# TrafficComposer

The official GitHub repository for ICSE 2025 submission #179 Multi-Modal Traffic Scenario Generation for Autonomous Driving System Testing.

## Multi-Modality Traffic Scenario IR Generation

TrafficComposer extracts information from both textual and visual inputs and merges the information into a comprehensive traffic IR.

Users need to run the following procedures, and set the corresponding file paths in `main.py` before running `main.py`.

### 1. Extract information from the textual description:

```
cd gen_traffic_ir/gen_textual_ir
python gpt_text_parser.py
```

By default, TrafficComposer uses GPT-4 (gpt-4-1106-preview).

Then specify the `dir_text_ir` in `main.py` as the path to the generated textual_ir.

### 2. Extract information from the reference image:

#### 2.1 Extract lane divisions from the reference image:

TrafficComposer proposes a visual information extractor using two pre-trained computer vision models, [YOLOv8](https://github.com/ultralytics/ultralytics) and [CLRNet](https://github.com/Turoad/CLRNet).

You first need [CLRNet](https://github.com/Turoad/CLRNet) to extract the lane divisions. The lane division results should be stored at `dir_lane_detection_file`, which is used by the visual information extractor in `main.py`.

If user uses an image from a dataset inherently supported by CLRNet ([CULane](https://xingangpan.github.io/projects/CULane.html), [Tusimple](https://github.com/TuSimple/tusimple-benchmark/issues/3), and [LLAMAS](https://unsupervised-llamas.com/llamas/)), user can directly follow CLRNet's instructions for the corresponding dataset.
Most cases in our benchmark uses images from the [CULane](https://xingangpan.github.io/projects/CULane.html) dataset.
(Please note to select videos from the test set or validation set, as we did in our experiments. This approach is to reveal CLRNet's real performance level in reference time and avoid the risk of overfitting to the training set.)

We also create a script for users to support images not in the three datasets.
To run CLRNet, user firstly needs to put the `main_clrnet.py` in the CLRNet root directory, download the pre-trained weight files, and run with the arguments as instructed in CLRNet README. 

The following is an example bash command to run the script. User can specify the pretrained model by specifying the config file and the `--load_from` weight file.
User may need to adjust the parameters regarding the image size in the configuration.

```
python main_clrnet.py configs/clrnet/clr_dla34_culane.py --validate --load_from culane_dla34.pth --gpus 0 --view
```

Then, specify the `dir_lane_detection_file` in `main.py` as the path to the output of CLRNet.

#### 2.2 Detect vehicles and pedestrains from the reference image:

To extract the positions of actors in the reference image, you need [YOLOv8](https://github.com/ultralytics/ultralytics) to detect each actor in the reference image.
The vehicle detection results should be stored at `dir_vehicle_detection_file`, which is used by the visual information extractor in `main.py`.

Then, specify the `dir_vehicle_detection_file` in `main.py` as the path to the output of YOLOv8.

### 3. Align the information from the two modalities to a comprehensive traffic IR.

```
python main.py
```

## Run the ADS fuzzing methods with the traffic scenarios generated with TrafficComposer

TrafficComposer automatically converts the traffic IR to executable traffic scenarios.

To evaluate the effectiveness of TrafficComposer, we experiment with two state-of-the-art ADS fuzzing methods, DriveFuzz and LawBreaker, leveraging scenario seeds generated by TrafficComposer.

To run fuzz testing on ADSs, you first need to install [DriveFuzz](https://github.com/dk-kling/drivefuzz?tab=readme-ov-file) and [LawBreaker](https://github.com/lawbreaker2022/LawBreaker-SourceCode/).

Then, to run fuzz testing with scenario seeds generated by TrafficComposer:

For DriveFuzz:

We extended the original [DriveFuzz](https://github.com/dk-kling/drivefuzz?tab=readme-ov-file) by adding support to more ADSs under test by combining the [CARLA-expert Project](https://github.com/Kin-Zhang/carla-expert).

We publish our extended DriveFuzz source code in `fuzzing/drivefuzz-expert/`.

Users can bootstrap the simulation by

```
cd fuzzing/drivefuzz-expert/
python trafficcomposer_drivefuzz.py -o <out-artifact> -s <seed-artifact> -c 5 -m 5 -t expert --timeout 60 --town 1 --strategy all
```

The generated scenario folder should be stored in the same directory along with the `opendriveparser.py` from Carla. The input seeds will be stored at `<seed-artifact>`, and the record of the fuzzing will be stored at `<out-artifact>`. You can set the town map of the corresponding carla configuration using the argument `--town` and time out wait time with `--time out`. You can choose agents with the argument `-t` with the option `autoware, behavior, auto, roach, transfuser, mmfn`.

For LawBreaker:

Firstly, convert the traffic scenario IR stored in YAML files to AVUnit by

```
cd fuzzing/trafficcomposer_lawbreaker/
python trafficcomposer_lawbreaker.py input.yaml bnf.txt
```

You can use the following bash script to process the transformation of seeds in batch.

```
#!/bin/bash

output_dir="../bnf"

if [ ! -d "$output_dir" ]; then
    mkdir -p "$output_dir"
fi

for file in *; do
    # Skip if it's a directory
    if [ -d "$file" ]; then
        continue
    fi
    output_file="${file%.yaml}_bnf.txt"

    python trafficcomposer_lawbreaker.py "$file" "$output_dir/$output_file"
done
```

Then, move the converted file (`bnf.txt`) under `test_cases/traffic_rule_tests/`  of the configured LawBreaker project directory.

Start the fuzz testing by

```
cd fuzzing/trafficcomposer_lawbreaker/LawBreaker-SourceCode
python Law_Breaking_Fuzzing.py
```

## Comparison Baseline Implementation

### Tree Edit Distance 

In the evaluation of RQ2, RQ3, and RQ4, we use a Tree Edit Distance to measure the information accuracy.

```
cd gen_traffic_ir/
python cal_ir_distance.py
```

### RQ2. Information Extraction Accuracy

To evaluate RQ2, we experimented with two existing baselines:
[AC3R](https://github.com/TriHuynh00/AC3R-Demo)
[RMT](https://github.com/ITSEG-MQ/RMT-TSE)

Besides, we implemented a GPT-4V baseline:

```
cd gen_traffic_ir/baseline_GPT-4V
python gpt4_multi_modality.py
```

### RQ3. Contribution of each Modality

To evaluate RQ3, we calculated the tree edit distance of the information extracted from each modality to the ground truth traffic IR.

### RQ4. Generalizability to Different LLMs

Besides GPT-4, we also experimented with Llama-2 of three different sizes, including llama-2-70b-chat, llama-2- 13b-chat, and llama-2-7b-chat.

```
cd gen_textual_ir
python llama_text_parser.py
```

## Traffic Scenario Benchmark

We created 80 traffic scenarios with textual descriptions, reference images of the traffic scenario, and the ground truth traffic IR.

We also created the ground truth textual IR and the ground truth Visual IR for each traffic scenarios, although they were NOT used in our experiments.

The data can be accessed in `benchmark/`.
